# JOB CONFIGURATION
wandb_project_folder: "CIFAR-100_centralized"         # Name of the project folder in wandb
seed: 42                                              # Random seed for reproducibility

# centralized training config
batch_size: 256
val_split: 0.2
num_workers: 4
learning_rate: 0.008805
weight_decay: 0.01
momentum: 0.9
t_max: 50
epochs: 50
checkpoint_path: "/content/drive/MyDrive/DL_project/checkpoints/checkpoint_35.pth"           # Leave blank "" to train from scratch, or set to "checkpoints/<checkpoint_filename>.pth" to resume
out_checkpoint_dir: "/content/drive/MyDrive/DL_project/checkpoints"

  # sparse fine-tuning (centralized)
enable_sparse_finetuning: True          # Enable sparse fine-tuning
target_sparsity: 0.8                    # Target sparsity level (e.g., 80%)
sparsity_rounds: 5                      # Number of rounds for mask calibration
calib_split: 0.05                       # split of the training data dedicated to mask calibration

# federated training config
NUM_CLIENTS: 100
CLIENT_FRACTION: 0.1  # 10% of clients selected each round
LOCAL_EPOCHS: 4
BATCH_SIZE: 32
LR: 0.01
ROUNDS: 100
IID: True  # Set to False for non-iid
NC:   # Number of classes per client in non-iid setting
SEED: 42
