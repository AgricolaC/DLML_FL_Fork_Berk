# --------------------------
# JOB CONFIGURATION
# --------------------------

wandb_project_folder: "CIFAR-100_centralized"         # Name of the project folder in wandb
seed: 42                                              # Random seed for reproducibility

# --------------------------
# CENTRALIZED TRAINING CONFIG
# --------------------------
batch_size: 256
val_split: 0.1
num_workers: 2
learning_rate: 0.003214
weight_decay: 0.00005743
momentum: 0.96
t_max: 50
epochs: 50
checkpoint_path: '' # "/content/drive/MyDrive/DL_project/checkpoints/checkpoint_35.pth"           # Leave blank "" to train from scratch, or set to "checkpoints/<checkpoint_filename>.pth" to resume
out_checkpoint_dir: "/content/drive/MyDrive/DL_project/checkpoints"

finetuning_method: "dense"                             # fine-tuning method switch -> "dense" / "lora" / "talos"

# talos fine-tuning (centralized)
nesterov: False                         # enable nesterov acceleration
dampening: 0                            # fraction of grad to drop from momentum buffer
target_sparsity: 0.8                    # Target sparsity level (e.g., 80%)
sparsity_rounds: 5                      # Number of rounds for mask calibration
calib_split: 0.05                       # split of the training data dedicated to mask calibration
calib_batch_size: 16                    # for Fisher calibration
calib_rounds: 5

# LoRA fine-tuning (centralized)
lora_rank: 8                                                                # dimensionality of the low-rank update matrices (AB)
lora_alpha: 32                                                              # factor applied to the low-rank update (alpha/r * AB)
lora_dropout: 0.05                                                          # Dropout probability on the updates
lora_target_modules:
  - "qkv"
  - "proj"
  # optionally, for adapters in the MLP:
  # - "mlp.fc1"
  # - "mlp.fc2"
# --------------------------
# FEDERATED TRAINING CONFIG
# --------------------------
NUM_CLIENTS: 100
CLIENT_FRACTION: 0.1  # 10% of clients selected each round
LOCAL_EPOCHS: 4
BATCH_SIZE: 32
LR: 0.01
ROUNDS: 100
IID: True  # Set to False for non-iid
NC:   # Number of classes per client in non-iid setting
SEED: 42
